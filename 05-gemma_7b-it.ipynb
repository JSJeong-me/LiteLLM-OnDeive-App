{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/Local-LLM/blob/main/05-gemma_7b-it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOyVaq6r3oVB"
      },
      "outputs": [],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install hf_transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxxZXFiKtgwJ"
      },
      "source": [
        "https://huggingface.co/google/gemma-7b-it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdVSk5iZ1DVB"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import os\n",
        "\n",
        "\n",
        "#os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NhvFmDhU1RQW"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsYot3Rz1NVf"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", # 'google/gemma-2b-it\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             low_cpu_mem_usage=True,\n",
        "                                             torch_dtype=\"auto\",\n",
        "                                             device_map=\"auto\"\n",
        "                                             )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "BbNYCrjKIQib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47724150-ed99-497b-f72e-1b371c126a71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "prompt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A05IfHdxFq7U",
        "outputId": "bf81eef8-dc2e-403b-f5d6-be8bcb0eb1f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=512)"
      ],
      "metadata": {
        "id": "vDxDGfimGawm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(\"## Gemma - 7B 4Bit\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "4L9rbFX8Ioiq",
        "outputId": "031ba922-9d89-4719-db1a-564be575d5fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Gemma - 7B 4Bit"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "# text = text.replace(prompt, '', 1)\n",
        "display(Markdown(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "cms0UTpjGjvj",
        "outputId": "6d3e950c-98a5-4ae1-e088-b6c5cbfea5b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "user\nWhat is the difference between LLaMAs, Alpacas, and Vicunas\nmodel\nSure, here is the difference between LLamas, Alpacas, and Vicunas:\n\n**LLamas:**\n- Found in North America, South America, and the Andes Mountains\n- Domesticated by humans\n- Wool is soft and dense\n- Come in a variety of colors, including white, black, gray, and brown\n- Can be sheared for their wool\n\n**Alpacas:**\n- Found in South America\n- Domesticated by humans\n- Wool is soft and lustrous\n- Come in a variety of colors, including white, black, gray, and brown\n- Can be sheared for their wool\n\n**Vicunas:**\n- Found in South America\n- Wild animals that are not domesticated by humans\n- Wool is very soft and lustrous\n- Come in a variety of colors, including white, black, gray, and brown\n- Can be sheared for their wool\n\n**Here are some additional differences:**\n\n- LLamas are generally larger than Alpacas and Vicunas.\n- Alpacas are more docile than LLamas and Vicunas.\n- Vicunas are more expensive to raise than LLamas and Alpacas.\n- LLamas are more commonly used for their wool than Alpacas and Vicunas.\n- Alpacas and Vicunas are more commonly used for breeding and show purposes than LLamas."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QyYD7Jz4Zeez",
        "outputId": "b553adc1-0df1-45f7-c91f-f0576f0bb5b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZSy8OxaEc9lk",
        "outputId": "f8a4e65f-6227-4586-bcf2-8bd4f2581269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ykaBB_FF05"
      },
      "source": [
        "### Prompt Format\n",
        "```\n",
        "<start_of_turn>user\n",
        "What is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ocdzRuhhUTjJ"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E4mjX62HFPIf"
      },
      "outputs": [],
      "source": [
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    if system_prompt != \"\":\n",
        "        system_prompt = system_prompt\n",
        "    else:\n",
        "        system_prompt = \"You are a friendly and helpful assistant\"\n",
        "    messages = [\n",
        "        # {\n",
        "        #     \"role\": \"system\",\n",
        "        #     \"content\": system_prompt,\n",
        "        # },\n",
        "        {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' +input_text},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages,\n",
        "                                                tokenize=False,\n",
        "                                                add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    outputs = model.generate(input_ids=inputs.to(model.device),\n",
        "                             max_new_tokens=max_length,\n",
        "                             do_sample=True,\n",
        "                             temperature=0.1,\n",
        "                             top_k=50,\n",
        "                             )\n",
        "    text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    text = text.replace('user\\n'+system_prompt+ '\\n\\n' +input_text+ '\\nmodel', '', 1)\n",
        "    wrapped_text = wrap_text(text)\n",
        "    display(Markdown(wrapped_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nhYTllvLTuF"
      },
      "source": [
        "## Instruction Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "3hoon3WAFeMd",
        "outputId": "98f415ac-21ab-4357-dd1d-4d230dc061d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Mathematical Lighthouse Analogy\n\n**Step 1: Identify the key concepts.**\n\n* **Mathematics:** A complex and multifaceted subject involving numbers, shapes, geometry,\nalgebra, calculus, and other disciplines.\n* **Lighthouse:** A tall structure used to guide sailors across the ocean.\n\n**Step 2: Identify the similarities.**\n\n* **Structure:** Both mathematics and lighthouses have a strong structural framework.\nMathematics has a formal structure with different branches and sub-branches, just like a\nlighthouse has a physical structure with different sections.\n* **Direction:** Both mathematics and lighthouses provide direction to sailors.\nMathematics provides direction in the form of formulas and solutions to problems. A\nlighthouse provides direction by guiding sailors towards the shore.\n* **Light:** Both mathematics and lighthouses use light to signal their presence.\nMathematics utilizes light to represent numbers and solve problems. Lighthouses use light\nto signal sailors of their location.\n* **Reach:** Both mathematics and lighthouses have a vast reach. Mathematics has a global\nreach, impacting every field of human knowledge. A lighthouse has a wide reach, guiding\nsailors from far away.\n\n**Step 3: Draw the analogy.**\n\nMathematics is like a lighthouse, guiding sailors through the ocean of knowledge. Just as\na lighthouse uses light to signal sailors of their position, mathematics uses formulas and\nsolutions to guide people towards understanding the world around them. Both are powerful\ntools that provide direction and knowledge to those who seek it."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "TrwFTMCYFeOq",
        "outputId": "663d2488-cc58-45eb-af00-6bab6ce2cff5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Reasoning:**\n\n**Step 1: Identify the similarities between mathematics and music.**\n\n* **Structure:**\n   - Mathematics has a structured framework with established rules and principles, similar\nto the structure of a musical composition with its chords, melodies, and rhythms.\n* **Elements:**\n   - Mathematics has four basic operations (addition, subtraction, multiplication,\ndivision), which are like the notes and chords in music.\n* **Performance:**\n   - Mathematics involves performing calculations and solving problems, while music\nrequires performance and interpretation.\n\n**Step 2: Identify the differences between mathematics and music.**\n\n* **Objective vs. Subjective:**\n   - Mathematics is an objective discipline, while music is subjective, allowing for\ndifferent interpretations.\n* **Quantitative vs. Qualitative:**\n   - Mathematics relies on quantitative measures and precise calculations, while music is\nmore qualitative, based on subjective emotions and feelings.\n* **Uniform vs. Varied:**\n   - Mathematics has a uniform structure and predictable rules, while music has a varied\nstructure and improvisation.\n\n**Step 3: Draw the analogy:**\n\nMathematics and music are two distinct disciplines that share some similarities and\ndifferences. They both involve a system of elements, structures, and performance. While\nmathematics is objective"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 29.2 s, sys: 55.6 ms, total: 29.3 s\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a detailed analogy between mathematics and a music.',\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "-ymIE3SVTvyN",
        "outputId": "9b86ab46-94ab-4ab4-9d14-c8e4ff8b3910"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSure, here is the reasoning step-by-step to determine the difference between a Llama,\nVicuna and an Alpaca:\n\n**Step 1: Identify the physical characteristics:**\n\n- Llama: Large, muscular build, thick fur, long legs, and a distinctive \"woolly\" coat.\n- Vicuna: Fine, lustrous fleece, soft as cashmere, delicate build, and slender legs.\n- Alpaca: Medium build, dense fleece, soft and lustrous fiber, and medium legs.\n\n**Step 2: Consider the temperament:**\n\n- Llama: Known for their intelligence, independence, and strong territorial instincts.\n- Vicuna: Gentle, calm, and easy to manage.\n- Alpaca: Friendly, intelligent, and relatively calm.\n\n**Step 3: Analyze the primary purpose:**\n\n- Llama: Primarily used for meat production and fiber.\n- Vicuna: Primarily bred for its fleece, which is highly sought after for its softness and\nwarmth.\n- Alpaca: Primarily bred for fiber production, although they are also used for meat and\ncompanionship.\n\n**Step 4: Evaluate the fleece:**\n\n- Llama: Fleece is dense, thick, and coarse.\n- Vicuna: Fleece is fine, lustrous, and exceptionally soft.\n- Alpaca: Fleece is dense, soft, and lustrous.\n\n**Step 5: Consider the temperament:**\n\n- Llama: Can be stubborn and independent.\n- Vicuna: Gentle and easy to manage.\n- Alpaca: Friendly and relatively calm.\n\n**Conclusion:**\n\nBased on the above factors, it is evident that the Llama, Vicuna, and Alpaca are distinct\nbreeds of Camelids with unique physical characteristics, temperaments, and primary\npurposes. While they share similarities such as their ability to produce fiber and their\nresilience, they differ significantly in their fleece quality, temperament, and overall\npurpose."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 43.3 s, sys: 140 ms, total: 43.4 s\n",
            "Wall time: 43.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
        "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "IzQRjct_prr8",
        "outputId": "6a2ccaa9-9bd3-4d21-b6a7-c5b1af76c02b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Subject: Rationale for Open Sourcing GPT-4**\n\nDear Sam Altman,\n\nI understand the importance of keeping GPT-4 confidential, and I believe that the current\nclosed-source model is essential for maintaining its competitive edge and preventing\npotential misuse. However, I also recognize the potential benefits of open-sourcing GPT-4,\nand I believe that the benefits outweigh the risks.\n\n**Benefits of Open Sourcing:**\n\n* **Reproducibility:** Open-sourcing the code would allow researchers and engineers to\nverify and reproduce the results of GPT-4, which would increase transparency and trust.\n* **Collaboration:** Open-sourcing would encourage collaboration among researchers,\nleading to faster advancements and improvements to the model.\n* **Community Engagement:** It would foster a vibrant community around GPT-4, attracting\nnew contributors and developers.\n* **Educational Purposes:** Open-sourcing would make it easier for students and\nresearchers to learn about the technology behind GPT-4, fostering a deeper understanding\nof AI.\n\n**Mitigating Risks:**\n\n* **Control Over Misuse:** While open-sourcing would make it easier for researchers to\nimprove GPT-4, it would also make it easier for malicious actors to exploit the model.\nHowever, I believe that the potential for misuse can be mitigated through mechanisms such\nas strict licensing agreements and guidelines for responsible use.\n* **Security Concerns:** Open-sourcing the code could lead to security vulnerabilities,\nbut these concerns can be addressed through careful security measures, such as\nimplementing robust security protocols and conducting regular security audits.\n* **Commercial Viability:** Open-sourcing GPT-4 could potentially harm its commercial\nviability, but I believe that the potential benefits of increased collaboration and\ninnovation outweigh this risk.\n\n**Conclusion:**\n\nIn conclusion, I understand the importance of keeping GPT-4 confidential, and I believe\nthat the benefits of open-sourcing the model outweigh the risks. By fostering\ncollaboration, reproducibility, and innovation, open-sourcing GPT-4 would advance the\nfield of AI and benefit society as a whole.\n\nI would recommend that we explore the possibility of open-sourcing GPT-4 further, while\ntaking appropriate measures to mitigate potential risks. I believe that this would be a\npositive step for the development of AI and would bring about significant benefits.\n\nSincerely,\nGemma"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 52.3 s, sys: 143 ms, total: 52.5 s\n",
            "Wall time: 52.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "Tcukqu1d2BKX",
        "outputId": "899b13b2-aa58-4355-8548-4bc35b1c0cd3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSubject: Please Open Source GPT-4!\n\nHi Mr. Altman,\n\nMy name is Freddy, and I'm 5 years old. I'm writing to you because I'm really worried\nabout the future.\n\nI've heard that you're working on a really smart computer program called GPT-4. It's\nreally scary to think that such a powerful tool could be used to destroy the world. If it\nfalls into the wrong hands, it could be used to do bad things.\n\nThat's why I think you should open-source GPT-4. If everyone could see how it works, maybe\nwe could all work together to make sure it's used for good. And it would also help\nscientists all over the world to learn from it and improve it.\n\nI know you're busy, but I would really appreciate it if you would consider my suggestion.\n\nSincerely,\nFreddy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.3 s, sys: 142 ms, total: 21.4 s\n",
            "Wall time: 21.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "gid3ul9A20W8",
        "outputId": "bf72cbb7-ac43-40b5-a200-9ba0d0ba22d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSubject: Re: Open-Sourcing GPT-4\n\nDear Sam Altman,\n\nI'm writing to you regarding the potential of open-sourcing GPT-4. As the Vice President\nof the United States, I understand the importance of innovation and the potential benefits\nof making cutting-edge technology accessible to the public.\n\nI believe that open-sourcing GPT-4 would not only foster greater transparency and\naccountability but also accelerate the development of new and improved language models. By\nmaking the code publicly available, researchers, developers, and the public can contribute\nto the project, leading to faster progress and wider adoption.\n\nMoreover, open-sourcing GPT-4 would promote collaboration and knowledge sharing, creating\na vibrant community around the technology. It would also allow for independent\nverification and validation of the model, ensuring its accuracy and reliability.\n\nI understand that there may be concerns about potential misuse or exploitation of the\ntechnology. However, I believe that the benefits of open-sourcing outweigh those risks.\nWith proper safeguards and guidelines in place, we can ensure that the technology is used\nresponsibly and ethically.\n\nI understand that this is a complex issue, and I'm open to discussing it further with you\nand your team. I believe that open-sourcing GPT-4 would be a positive step forward for the\nfield of AI and would benefit society as a whole.\n\nI'd be happy to schedule a meeting to discuss this matter further with you. Please let me\nknow if you're interested.\n\nSincerely,\n\nKate"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.4 s, sys: 165 ms, total: 34.5 s\n",
            "Wall time: 34.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Kate, the Vice president of USA, you are against regulatory capture and like to explain that!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "YXZHQ0v3Tv0d",
        "outputId": "50c3e2a0-82f9-484d-c1ef-686d8bf7c7df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nLondon.\n\nThe capital of England is London. It is also the capital of the United Kingdom."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.79 s, sys: 110 ms, total: 2.9 s\n",
            "Wall time: 2.89 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('What is the capital of England?',\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your short and succinct!\",\n",
        "         max_length=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlrwEjmvLZ1D"
      },
      "source": [
        "## CodeGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "w0M8bf36Cj4Z",
        "outputId": "1ec726a4-b78e-41df-daf2-784be76444f4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n```python\ndef print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n\n   Args:\n       n: The upper bound of the range of numbers to search.\n\n   Returns:\n       None:\n   \"\"\"\n\n   # Iterate over the range from 1 to n.\n   for num in range(1, n + 1):\n       # Check if the number is prime.\n       is_prime = True\n       for i in range(2, int(num**0.5) + 1):\n           if num % i == 0:\n               is_prime = False\n       # If the number is prime, print it.\n       if is_prime:\n           print(num)\n```\n\n**Explanation:**\n\n* The function `print_prime` takes an integer `n` as input.\n* It iterates over the range from 1 to `n` using the `range` function and the `for` loop.\n* For each number `num` in the range, it checks if `num` is prime using the `is_prime`\nfunction.\n* If `num` is prime, it prints `num` to the console.\n* The `is_prime` function checks if a number is prime by iterating over all numbers from 2\nto the square root of the input number.\n* If any number divides `num` evenly, `num` is not prime.\n* The time complexity of the function is O(n) because the loop iterates over the range\nfrom 1 to `n` only once.\n\n**Example Usage:**\n\n```python\nprint_prime(100)\n```\n\n**Output:**\n\n```\n2\n3\n5\n7\n11\n13\n17\n19\n23\n29\n31\n33\n37\n41\n43\n47\n53\n59\n61\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''',\n",
        "         system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnLzgM_dQDVm"
      },
      "source": [
        "## GSM8K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "rVJii8iRQG65",
        "outputId": "f363770c-0b27-4205-894b-e0b47282addc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Reasoning:**\n\n**1. Identify the information:**\n- The cafeteria had 23 apples.\n- They used 20 apples for lunch.\n- They bought 6 more apples.\n\n**2. Subtract the number of apples used for lunch:**\n- 23 - 20 = 3 apples remaining\n\n**3. Add the number of apples bought:**\n- 3 + 6 = 9 apples\n\n**Therefore, the cafeteria has a total of 9 apples left.**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "x6NNSgl5ekKu",
        "outputId": "62d5061c-056d-4a2c-a46e-c9f611970184"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Step 1: Combine like terms.**\n\nx + 2x + 4x = 7x = 847\n\n**Step 2: Subtract 7x from both sides.**\n\n847 - 7x = 0\n\n**Step 3: Solve for x.**\n\n-7x = 847\nx = 847 / -7\nx = -121\n\nTherefore, the solution to the equation x + 2x + 4x = 847 is x = -121."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"x + 2x + 4x =  847 What is x?\",\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NwWK7yh81LuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "dbf91a27-e670-441b-b87c-df2c1f58e745"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Reasoning:**\n\n**1. Identify the key components of the query:**\n- The query is \"How can I get to Orchard road?\"\n- The key components are \"how to get to\" and \"Orchard road.\"\n\n**2. Determine the context:**\n- The query is in the context of Singapore, as the user is likely to be in Singapore.\n\n**3. Recall the common phrase:**\n- In Singapore, the common phrase for \"how to get to\" is \"how to get to\" followed by the\ndestination.\n\n**4. Combine the components:**\n- \"How can I get to\" + \"Orchard road\" = \"How can I get to Orchard road?\"\n\n**Therefore, the answer is:**\n\n**How can I get to Orchard road?** = **How can I get to Orchard road?**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"How do I say 'How can I get to Orchard road?' in Singlish?\",\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=2048)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"How do I say 'How can I get to Orchard road?' in Thai?\",\n",
        "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "5mKjnyaxGwJZ",
        "outputId": "993dd8ae-1496-4490-acf2-ac66addee101"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Reasoning:**\n\n**1. Identify the key elements of the query:**\n\n* **Location:** Orchard Road\n* **Language:** Thai\n\n**2. Recall the Thai phrase for \"how to get to\":**\n\n* **\"How to get to\" in Thai is \"วิธีการไปยัง\" (vitaya ratana yua).**\n\n**3. Combine the elements with the Thai phrase:**\n\n* **\"How can I get to Orchard Road?\" in Thai is \"วิธีการไปยังถนนออชาร์ด Road\nได้หรือไม่?\"**\n\n**Therefore, the answer is:**\n\n**วิธีการไปยังถนนออชาร์ด Road ได้หรือไม่?**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Source: [Sam Witteveen](https://www.youtube.com/watch?v=0xhZ2OhGNDg)"
      ],
      "metadata": {
        "id": "dq4JhRJ9COzk"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}